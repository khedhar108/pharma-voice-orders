# Pharma Voice Orders - Deployment Guide

This guide explains how to deploy and configure the application for production use with large AI models.

---

## üöÄ Deployment Options Comparison

| Feature | Streamlit Cloud (Deploy Button) | Hugging Face Spaces |
|---------|--------------------------------|---------------------|
| **Ease of Use** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê One-click | ‚≠ê‚≠ê‚≠ê‚≠ê Simple |
| **Free Tier** | 1GB RAM, limited | 16GB RAM (with GPU upgrade) |
| **GPU Support** | ‚ùå No | ‚úÖ Yes (paid: T4, A10G) |
| **Large Models (Whisper Medium+)** | ‚ö†Ô∏è May timeout | ‚úÖ Works well |
| **Privacy/Secrets** | ‚úÖ Secrets Manager | ‚úÖ Secrets Manager |
| **Best For** | Quick demos (tiny model) | Production + Large Models |

---

## üì± Option 1: Streamlit Cloud (The "Deploy" Button)

The **Deploy** button in your localhost Streamlit UI deploys directly to **Streamlit Community Cloud**.

### How It Works:
1. Click **Deploy** ‚Üí **Streamlit Community Cloud**
2. Connect your GitHub account
3. Select your repository and branch
4. Streamlit Cloud builds and hosts your app

### ‚ö†Ô∏è Limitations for Your Use Case:
- **1GB RAM limit** on free tier ‚Üí Whisper Medium (3GB) will **fail**
- **No GPU** ‚Üí Slow inference
- **Good for**: Demo with `whisper-tiny` only

### Setup:
```bash
# Push your code to GitHub first
git add .
git commit -m "Deploy to Streamlit Cloud"
git push origin main
```
Then click **Deploy** in the Streamlit UI.

---

## ‚òÅÔ∏è Option 2: Hugging Face Spaces (Recommended)

**Best for**: Large models (Whisper Medium, Large, Google Med SR) with HF Token.

### Step-by-Step Deployment:

#### 1. Create a Hugging Face Space
1. Go to [huggingface.co/spaces](https://huggingface.co/spaces)
2. Click **Create new Space**
3. Select:
   - **SDK**: Streamlit
   - **Hardware**: CPU Basic (free) or upgrade for GPU
   - **Visibility**: Public or Private

#### 2. Create `app.py` (Already Done ‚úÖ)

#### 3. Create `requirements.txt` for HF Spaces
Create a file **specifically for Spaces** (different from local):

```txt
streamlit
pandas
openpyxl
torch
transformers
librosa
noisereduce
soundfile
rapidfuzz
jiwer
regex
webrtcvad
numpy<2
huggingface_hub
```

#### 4. Add Your HF Token as a Secret
1. Go to your Space ‚Üí **Settings** ‚Üí **Repository secrets**
2. Add a new secret:
   - **Name**: `HF_TOKEN`
   - **Value**: Your Hugging Face read token (from [hf.co/settings/tokens](https://huggingface.co/settings/tokens))

#### 5. Update Code to Use Token
In `core/asr_engine.py`, the model will automatically use `HF_TOKEN`:

```python
import os
from huggingface_hub import login

# Auto-login with Space secret
token = os.environ.get("HF_TOKEN")
if token:
    login(token=token)
```

#### 6. Push Code to the Space
```bash
# Clone your Space
git clone https://huggingface.co/spaces/YOUR_USERNAME/pharma-voice-orders
cd pharma-voice-orders

# Copy your files
cp -r /path/to/your/local/project/* .

# Push
git add .
git commit -m "Initial deployment"
git push
```

---

## üîë Using Gated Models (Google Med SR, etc.)

Some models require you to accept terms on the model page before using.

### Steps:
1. Visit the model page (e.g., `google/med-sr-model`)
2. Click **Agree and access model**
3. Add your `HF_TOKEN` to the Space secrets (as shown above)
4. Update your code to specify the model ID:

```python
# In core/asr_engine.py
model_id = "google/med-speech-recognition"  # Example
```

---

## üéØ Recommended Strategy for Your Project

| Phase | Platform | Model | Why |
|-------|----------|-------|-----|
| **Development** | Local (`uv run start`) | `whisper-tiny` | Fast iteration |
| **University Demo** | Hugging Face Spaces (Free CPU) | `whisper-small` | Balance of quality + speed |
| **Production Demo** | HF Spaces + GPU (T4) | `whisper-medium` or Google Med SR | Best quality |

---

## üîÑ Pre-Caching Models (Avoid First-Run Download)

To make the model load instantly for visitors, add a **pre-download script** in your Space:

Create `preload.py`:
```python
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor

# Pre-download during build
model_id = "openai/whisper-medium"
AutoModelForSpeechSeq2Seq.from_pretrained(model_id)
AutoProcessor.from_pretrained(model_id)
print("Model pre-cached!")
```

Then add to your Space's `README.md`:
```yaml
---
title: Pharma Voice Orders
sdk: streamlit
sdk_version: 1.53.0
app_file: app.py
pinned: false
preload: preload.py
---
```

---

## üìÅ Final File Structure for HF Spaces

```
pharma-voice-orders/
‚îú‚îÄ‚îÄ app.py                    # Main Streamlit app
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ preload.py               # Model pre-download script
‚îú‚îÄ‚îÄ README.md                # Space metadata (YAML frontmatter)
‚îú‚îÄ‚îÄ core/                    # Your modules
‚îú‚îÄ‚îÄ simulation/
‚îú‚îÄ‚îÄ evaluation/
‚îî‚îÄ‚îÄ data/
```

---

*Last Updated: January 2026*
